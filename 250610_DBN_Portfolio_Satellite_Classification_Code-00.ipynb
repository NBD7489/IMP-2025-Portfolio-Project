{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f833e86-9536-45d3-bcfd-7c02a2da3d6f",
   "metadata": {},
   "source": [
    "### Module 25 - Required Activity 25.3 - Portfolio Project\n",
    "\n",
    "##### DBN Submission (10/06/2025)\n",
    "\n",
    "###### Description: This algorithm classifies satellite images from the EuroSAT database\n",
    "\n",
    "###### EuroSAT Data Citation (Available at: https://github.com/phelber/eurosat [Accessed: 10/06/2025]):\n",
    "\n",
    "    [1] Eurosat: A novel dataset and deep learning benchmark for land use and land cover classification. Patrick Helber, Benjamin Bischke, Andreas Dengel, Damian Borth. IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing, 2019.\n",
    "    \n",
    "    @article{helber2019eurosat,\n",
    "      title={Eurosat: A novel dataset and deep learning benchmark for land use and land cover classification},\n",
    "      author={Helber, Patrick and Bischke, Benjamin and Dengel, Andreas and Borth, Damian},\n",
    "      journal={IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing},\n",
    "      year={2019},\n",
    "      publisher={IEEE}\n",
    "    }\n",
    "    [2] Introducing EuroSAT: A Novel Dataset and Deep Learning Benchmark for Land Use and Land Cover Classification. Patrick Helber, Benjamin Bischke, Andreas Dengel. 2018 IEEE International Geoscience and Remote Sensing Symposium, 2018.\n",
    "    \n",
    "    @inproceedings{helber2018introducing,\n",
    "      title={Introducing EuroSAT: A Novel Dataset and Deep Learning Benchmark for Land Use and Land Cover Classification},\n",
    "      author={Helber, Patrick and Bischke, Benjamin and Dengel, Andreas and Borth, Damian},\n",
    "      booktitle={IGARSS 2018-2018 IEEE International Geoscience and Remote Sensing Symposium},\n",
    "      pages={204--207},\n",
    "      year={2018},\n",
    "      organization={IEEE}\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92b65b33-d8d9-419e-a5ab-a8ae2211c0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a1a755-9c7f-431b-865e-cb879f62e9f0",
   "metadata": {},
   "source": [
    "##### Step 1 - Load DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7331e89f-e296-4f4e-971d-8a350eeaaab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 9.91M/9.91M [00:06<00:00, 1.42MB/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 28.9k/28.9k [00:00<00:00, 331kB/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 1.65M/1.65M [00:08<00:00, 191kB/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 4.54k/4.54k [00:00<00:00, 8.73MB/s]\n"
     ]
    }
   ],
   "source": [
    "# Load Data using Torch\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize(0.5, 0.5)])\n",
    "\n",
    "trainset = torchvision.datasets.MNIST('data/eurosat', train = True,\n",
    "                                        download=True, transform=transform)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=1,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.MNIST('data/eurosat', train = False,\n",
    "                                        download=True, transform=transform)\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=1,\n",
    "                                         shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8e5d0e-583e-4235-adbc-e0d05a754f84",
   "metadata": {},
   "source": [
    "##### Step 2 - Define the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3080b646-746c-4918-b2c1-4cc07c7ac049",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LetNet5: total params: 35068\n",
      "Net(\n",
      "  (conv1): Conv2d(1, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=(3, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(10, 14, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=168, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, name=None):\n",
    "        super(Net, self).__init__()\n",
    "        if name:\n",
    "            self.name = name\n",
    "        self.conv1 = nn.Conv2d(1, 10, (5,5))\n",
    "        self.pool = nn.MaxPool2d((3,2), (2,2))\n",
    "        self.conv2 = nn.Conv2d(10, 14, 5)\n",
    "        self.fc1 = nn.Linear(14*6*1*2, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "        # compute the total number of parameters\n",
    "        total_params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "        print(self.name + ': total params:', total_params)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 14*6*1*2)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "net = Net(name='LetNet5')\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4708e0df-d596-410b-80ca-ab1517dd80f7",
   "metadata": {},
   "source": [
    "##### Step 3 - Define Loss and Optimisation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a73576ce-1201-435d-825d-9daf2cab3782",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece87973-9f73-40a5-a616-672b22eb0d12",
   "metadata": {},
   "source": [
    "##### Step 4 - Train the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eed10239-cad1-4163-9aeb-e569e2a99433",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  2000] loss: 1.513\n",
      "[1,  4000] loss: 0.384\n",
      "[1,  6000] loss: 0.298\n",
      "[1,  8000] loss: 0.227\n",
      "[1, 10000] loss: 0.188\n",
      "[1, 12000] loss: 0.195\n",
      "[1, 14000] loss: 0.172\n",
      "[1, 16000] loss: 0.122\n",
      "[1, 18000] loss: 0.136\n",
      "[1, 20000] loss: 0.149\n",
      "[1, 22000] loss: 0.113\n",
      "[1, 24000] loss: 0.114\n",
      "[1, 26000] loss: 0.122\n",
      "[1, 28000] loss: 0.136\n",
      "[1, 30000] loss: 0.142\n",
      "[1, 32000] loss: 0.118\n",
      "[1, 34000] loss: 0.105\n",
      "[1, 36000] loss: 0.140\n",
      "[1, 38000] loss: 0.117\n",
      "[1, 40000] loss: 0.127\n",
      "[1, 42000] loss: 0.118\n",
      "[1, 44000] loss: 0.103\n",
      "[1, 46000] loss: 0.130\n",
      "[1, 48000] loss: 0.135\n",
      "[1, 50000] loss: 0.100\n",
      "[1, 52000] loss: 0.078\n",
      "[1, 54000] loss: 0.138\n",
      "[1, 56000] loss: 0.111\n",
      "[1, 58000] loss: 0.093\n",
      "[1, 60000] loss: 0.088\n",
      "[2,  2000] loss: 0.081\n",
      "[2,  4000] loss: 0.090\n",
      "[2,  6000] loss: 0.083\n",
      "[2,  8000] loss: 0.077\n",
      "[2, 10000] loss: 0.075\n",
      "[2, 12000] loss: 0.084\n",
      "[2, 14000] loss: 0.077\n",
      "[2, 16000] loss: 0.081\n",
      "[2, 18000] loss: 0.088\n",
      "[2, 20000] loss: 0.070\n",
      "[2, 22000] loss: 0.077\n",
      "[2, 24000] loss: 0.080\n",
      "[2, 26000] loss: 0.067\n",
      "[2, 28000] loss: 0.054\n",
      "[2, 30000] loss: 0.079\n",
      "[2, 32000] loss: 0.090\n",
      "[2, 34000] loss: 0.096\n",
      "[2, 36000] loss: 0.086\n",
      "[2, 38000] loss: 0.093\n",
      "[2, 40000] loss: 0.079\n",
      "[2, 42000] loss: 0.086\n",
      "[2, 44000] loss: 0.072\n",
      "[2, 46000] loss: 0.096\n",
      "[2, 48000] loss: 0.058\n",
      "[2, 50000] loss: 0.076\n",
      "[2, 52000] loss: 0.078\n",
      "[2, 54000] loss: 0.089\n",
      "[2, 56000] loss: 0.063\n",
      "[2, 58000] loss: 0.075\n",
      "[2, 60000] loss: 0.061\n",
      "Finished Training\n",
      "training time  455.215544462204\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for epoch in range(2):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "end = time.time()\n",
    "print('training time ', end-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7099c2ad-9aeb-4eb0-85f4-473d15fcc863",
   "metadata": {},
   "source": [
    "##### Step 5 - Test the Network on the Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e4107d8a-40c4-402f-8e6a-db92340ee825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 97 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
    "    100 * correct / total))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
